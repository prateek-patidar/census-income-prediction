{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import exp,array,random,dot\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(matrix):\n",
    "    n = matrix.shape[1]\n",
    "    norm_matrix = matrix\n",
    "\n",
    "    for i in range(n):\n",
    "        col_mean = np.mean(matrix[:,i])\n",
    "        col_stdv = np.std(matrix[:,i])\n",
    "    norm_matrix[:,i] = (norm_matrix[:,i] - col_mean) / col_stdv\n",
    "    return norm_matrix\n",
    "\n",
    "def normalize1(matrix):\n",
    "    return (matrix - matrix.mean()) / matrix.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading data from csv\n",
    "data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classifier data entries to dummy coloumns\n",
    "#data = pd.get_dummies(data)\n",
    "#data.to_csv(\"data/data_afterdum.csv\", sep=',',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27281 11692\n"
     ]
    }
   ],
   "source": [
    "# Spliting the data into Training data and Validation data\n",
    "train, validate = np.split(data.sample(frac=1), [int(.7*len(data))])\n",
    "print(len(train),len(validate))\n",
    "#print(train)\n",
    "#print(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27281, 45)\n"
     ]
    }
   ],
   "source": [
    "trn_ids = train[['id']]\n",
    "#print(train_ids)\n",
    "trn_y = train[['salary']]\n",
    "#print(train_y)\n",
    "trn_features = train.drop('id', axis=1)\n",
    "trn_features = trn_features.drop('salary', axis=1)\n",
    "trn_features = trn_features.drop('race', axis=1)\n",
    "trn_features = trn_features.drop('native-country', axis=1)\n",
    "trn_features = trn_features.drop('education', axis=1)#correlated\n",
    "trn_features = pd.get_dummies(trn_features)\n",
    "trn_features = normalize1(trn_features)\n",
    "print(trn_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11692, 45)\n"
     ]
    }
   ],
   "source": [
    "vld_ids = validate[['id']]\n",
    "#print(valid_ids)\n",
    "vld_y = validate[['salary']]\n",
    "#print(train_y)\n",
    "vld_features = validate.drop('id', axis=1)\n",
    "vld_features = vld_features.drop('salary', axis=1)\n",
    "vld_features = vld_features.drop('native-country', axis=1)\n",
    "vld_features = vld_features.drop('race', axis=1)\n",
    "vld_features = vld_features.drop('education', axis=1)\n",
    "vld_features = pd.get_dummies(vld_features)\n",
    "vld_features = normalize1(vld_features)\n",
    "print(vld_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6878, 45)\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "test_ids = test_data['id']\n",
    "#print(test_ids)\n",
    "tst_features = test_data.drop('id', axis=1)\n",
    "tst_features = tst_features.drop('native-country', axis=1)\n",
    "tst_features = tst_features.drop('race', axis=1)\n",
    "tst_features = tst_features.drop('education', axis=1)\n",
    "tst_features = pd.get_dummies(tst_features)\n",
    "tst_features = normalize1(tst_features)\n",
    "print(tst_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now Neural Network implementation starts\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x))\n",
    "\n",
    "def derv_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "def derv_tanh(x):\n",
    "        return (1.0 - np.square(np.tanh(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Number of nodes in each Layer\n",
    "        l1_nodes=45\n",
    "        l2_nodes=30\n",
    "        l3_nodes=30\n",
    "        l4_nodes=1\n",
    "\n",
    "        # Assigning random weights to Neural Network\n",
    "        self.weights1 = 2 * random.random((l1_nodes, l2_nodes)) -1\n",
    "        self.weights2 = 2 * random.random((l2_nodes, l3_nodes)) -1\n",
    "        self.weights3 = 2 * random.random((l3_nodes, l4_nodes)) -1\n",
    "\n",
    "    # Activation functions\n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def derv_tanh(self,x):\n",
    "        return (1.0 - (np.tanh(x))**2)\n",
    "\n",
    "    # Train neural network using backpropogation\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations,eta):\n",
    "        for i in range(number_of_training_iterations):\n",
    "            # print(i)\n",
    "            \n",
    "            # Forward Pass\n",
    "            a2 = self.tanh(dot(training_set_inputs, self.weights1))\n",
    "            a3 = self.tanh(dot(a2, self.weights2))\n",
    "            output = self.tanh(dot(a3, self.weights3))\n",
    "\n",
    "            # Backpopogation Pass\n",
    "            # Error for each layer\n",
    "            del4 = (training_set_outputs - output)*self.derv_tanh(output)\n",
    "            print(i,np.mean(np.square(del4)))\n",
    "            del3 = dot(self.weights3, del4.T)*(self.derv_tanh(a3).T)\n",
    "            del2 = dot(self.weights2, del3)*(self.derv_tanh(a2).T)\n",
    "\n",
    "            # Adjustments (gradients) for each layer\n",
    "            adjustment3 = eta*dot(a3.T, del4)\n",
    "            adjustment2 = eta*dot(a2.T, del3.T)\n",
    "            adjustment1 = eta*dot(training_set_inputs.T, del2.T)\n",
    "\n",
    "            # Adjusting weights accordingly\n",
    "            self.weights1 += adjustment1\n",
    "            self.weights2 += adjustment2\n",
    "            self.weights3 += adjustment3\n",
    "\n",
    "        return self.weights1,self.weights2,self.weights3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork1():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Number of nodes in each Layer\n",
    "        l1_nodes=45\n",
    "        l2_nodes=3\n",
    "        l3_nodes=1\n",
    "\n",
    "        # Assigning random weights to Neural Network\n",
    "        self.weights1 = 2 * random.random((l1_nodes, l2_nodes)) -1\n",
    "        self.weights2 = 2 * random.random((l2_nodes, l3_nodes)) -1\n",
    "\n",
    "    # Activation functions\n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def derv_tanh(self,x):\n",
    "        return (1.0 - (np.tanh(x))**2)\n",
    "\n",
    "    # Train neural network using backpropogation\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations,eta):\n",
    "        for i in range(number_of_training_iterations):\n",
    "            # print(i)\n",
    "            \n",
    "            # Forward Pass\n",
    "            a2 = self.tanh(dot(training_set_inputs, self.weights1))\n",
    "            output = self.tanh(dot(a2, self.weights2))\n",
    "\n",
    "            # Backpopogation Pass\n",
    "            # Error for each layer\n",
    "            del3 = (training_set_outputs - output)*self.derv_tanh(output)\n",
    "            print(i,np.mean(np.square(del3)))\n",
    "            del2 = dot(self.weights2, del3.T)*(self.derv_tanh(a2).T)\n",
    "\n",
    "            # Adjustments (gradients) for each layer\n",
    "            adjustment2 = eta*dot(a2.T, del3)\n",
    "            adjustment1 = eta*dot(training_set_inputs.T, del2.T)\n",
    "\n",
    "            # Adjusting weights accordingly\n",
    "            self.weights1 += adjustment1\n",
    "            self.weights2 += adjustment2\n",
    "\n",
    "        return self.weights1,self.weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.236981825864\n",
      "1 0.237508722084\n",
      "2 0.231891537542\n",
      "3 0.220231123033\n",
      "4 0.214493937166\n",
      "5 0.213385869198\n",
      "6 0.212570088948\n",
      "7 0.211800423432\n",
      "8 0.211115704478\n",
      "9 0.210489604867\n",
      "10 0.209895641298\n",
      "11 0.209318593777\n",
      "12 0.208750978347\n",
      "13 0.208189356182\n",
      "14 0.207632220975\n",
      "15 0.207078953074\n",
      "16 0.206529328348\n",
      "17 0.20598329491\n",
      "18 0.205440876495\n",
      "19 0.204902134043\n",
      "20 0.204367151322\n",
      "21 0.203836027592\n",
      "22 0.203308870093\n",
      "23 0.202785785124\n",
      "24 0.20226686975\n",
      "25 0.201752207208\n",
      "26 0.201241868182\n",
      "27 0.200735918138\n",
      "28 0.200234428825\n",
      "29 0.199737490506\n",
      "30 0.199245221171\n",
      "31 0.198757769691\n",
      "32 0.198275311481\n",
      "33 0.19779803723\n",
      "34 0.197326137008\n",
      "35 0.196859782952\n",
      "36 0.196399113704\n",
      "37 0.19594422306\n",
      "38 0.195495154235\n",
      "39 0.195051900001\n",
      "40 0.194614407957\n",
      "41 0.194182589371\n",
      "42 0.193756329716\n",
      "43 0.193335499116\n",
      "44 0.192919961387\n",
      "45 0.192509581036\n",
      "46 0.192104228095\n",
      "47 0.191703781027\n",
      "48 0.191308127927\n",
      "49 0.190917166199\n",
      "50 0.190530800854\n",
      "51 0.190148941718\n",
      "52 0.18977150014\n",
      "53 0.189398385951\n",
      "54 0.189029505475\n",
      "55 0.188664761065\n",
      "56 0.188304052208\n",
      "57 0.187947277842\n",
      "58 0.187594339336\n",
      "59 0.187245143558\n",
      "60 0.186899605647\n",
      "61 0.186557651233\n",
      "62 0.186219218032\n",
      "63 0.185884256804\n",
      "64 0.185552731723\n",
      "65 0.185224620211\n",
      "66 0.184899912278\n",
      "67 0.184578609426\n",
      "68 0.184260723189\n",
      "69 0.183946273386\n",
      "70 0.183635286182\n",
      "71 0.183327792062\n",
      "72 0.183023823828\n",
      "73 0.182723414708\n",
      "74 0.182426596675\n",
      "75 0.182133399043\n",
      "76 0.181843847381\n",
      "77 0.181557962785\n",
      "78 0.181275761496\n",
      "79 0.180997254831\n",
      "80 0.180722449376\n",
      "81 0.180451347373\n",
      "82 0.180183947214\n",
      "83 0.179920243977\n",
      "84 0.179660229943\n",
      "85 0.179403895053\n",
      "86 0.179151227271\n",
      "87 0.178902212845\n",
      "88 0.178656836461\n",
      "89 0.178415081305\n",
      "90 0.178176929032\n",
      "91 0.177942359672\n",
      "92 0.177711351481\n",
      "93 0.177483880771\n",
      "94 0.177259921718\n",
      "95 0.177039446187\n",
      "96 0.176822423566\n",
      "97 0.176608820645\n",
      "98 0.176398601528\n",
      "99 0.176191727591\n",
      "100 0.175988157493\n",
      "101 0.175787847232\n",
      "102 0.175590750243\n",
      "103 0.175396817538\n",
      "104 0.175205997873\n",
      "105 0.175018237944\n",
      "106 0.174833482589\n",
      "107 0.174651675016\n",
      "108 0.174472757017\n",
      "109 0.174296669191\n",
      "110 0.174123351157\n",
      "111 0.173952741752\n",
      "112 0.173784779228\n",
      "113 0.17361940142\n",
      "114 0.173456545905\n",
      "115 0.173296150153\n",
      "116 0.173138151646\n",
      "117 0.172982488006\n",
      "118 0.172829097096\n",
      "119 0.172677917128\n",
      "120 0.17252888676\n",
      "121 0.172381945196\n",
      "122 0.172237032282\n",
      "123 0.172094088611\n",
      "124 0.171953055622\n",
      "125 0.171813875702\n",
      "126 0.171676492282\n",
      "127 0.171540849941\n",
      "128 0.171406894492\n",
      "129 0.17127457307\n",
      "130 0.171143834215\n",
      "131 0.17101462794\n",
      "132 0.170886905799\n",
      "133 0.170760620945\n",
      "134 0.170635728179\n",
      "135 0.170512183997\n",
      "136 0.170389946628\n",
      "137 0.170268976067\n",
      "138 0.170149234105\n",
      "139 0.170030684346\n",
      "140 0.169913292228\n",
      "141 0.169797025028\n",
      "142 0.16968185186\n",
      "143 0.169567743665\n",
      "144 0.169454673189\n",
      "145 0.169342614946\n",
      "146 0.16923154517\n",
      "147 0.169121441755\n",
      "148 0.169012284175\n",
      "149 0.168904053393\n",
      "150 0.168796731758\n",
      "151 0.168690302887\n",
      "152 0.168584751533\n",
      "153 0.168480063448\n",
      "154 0.16837622523\n",
      "155 0.168273224168\n",
      "156 0.168171048085\n",
      "157 0.168069685167\n",
      "158 0.167969123808\n",
      "159 0.167869352445\n",
      "160 0.167770359412\n",
      "161 0.167672132791\n",
      "162 0.167574660287\n",
      "163 0.167477929113\n",
      "164 0.167381925894\n",
      "165 0.167286636599\n",
      "166 0.167192046488\n",
      "167 0.167098140089\n",
      "168 0.167004901207\n",
      "169 0.166912312955\n",
      "170 0.166820357819\n",
      "171 0.166729017748\n",
      "172 0.166638274281\n",
      "173 0.166548108692\n",
      "174 0.166458502158\n",
      "175 0.166369435945\n",
      "176 0.166280891598\n",
      "177 0.166192851117\n",
      "178 0.166105297119\n",
      "179 0.166018212957\n",
      "180 0.165931582791\n",
      "181 0.165845391613\n",
      "182 0.165759625222\n",
      "183 0.165674270164\n",
      "184 0.165589313667\n",
      "185 0.165504743581\n",
      "186 0.165420548355\n",
      "187 0.16533671705\n",
      "188 0.165253239395\n",
      "189 0.165170105888\n",
      "190 0.165087307918\n",
      "191 0.1650048379\n",
      "192 0.16492268942\n",
      "193 0.164840857357\n",
      "194 0.164759337988\n",
      "195 0.164678129072\n",
      "196 0.164597229894\n",
      "197 0.164516641279\n",
      "198 0.164436365577\n",
      "199 0.164356406614\n",
      "200 0.164276769623\n",
      "201 0.16419746115\n",
      "202 0.164118488956\n",
      "203 0.1640398619\n",
      "204 0.163961589829\n",
      "205 0.163883683464\n",
      "206 0.163806154299\n",
      "207 0.163729014499\n",
      "208 0.163652276812\n",
      "209 0.163575954488\n",
      "210 0.163500061206\n",
      "211 0.163424611006\n",
      "212 0.163349618219\n",
      "213 0.163275097407\n",
      "214 0.163201063294\n",
      "215 0.1631275307\n",
      "216 0.163054514465\n",
      "217 0.162982029374\n",
      "218 0.162910090075\n",
      "219 0.16283871099\n",
      "220 0.162767906229\n",
      "221 0.162697689497\n",
      "222 0.162628074003\n",
      "223 0.162559072374\n",
      "224 0.162490696566\n",
      "225 0.16242295779\n",
      "226 0.162355866437\n",
      "227 0.162289432022\n",
      "228 0.162223663129\n",
      "229 0.162158567371\n",
      "230 0.162094151362\n",
      "231 0.162030420694\n",
      "232 0.161967379929\n",
      "233 0.1619050326\n",
      "234 0.161843381213\n",
      "235 0.161782427265\n",
      "236 0.16172217126\n",
      "237 0.161662612736\n",
      "238 0.16160375029\n",
      "239 0.161545581615\n",
      "240 0.161488103535\n",
      "241 0.161431312041\n",
      "242 0.161375202338\n",
      "243 0.161319768885\n",
      "244 0.161265005444\n",
      "245 0.16121090513\n",
      "246 0.161157460459\n",
      "247 0.161104663406\n",
      "248 0.161052505451\n",
      "249 0.16100097764\n",
      "250 0.160950070636\n",
      "251 0.160899774774\n",
      "252 0.160850080111\n",
      "253 0.160800976484\n",
      "254 0.160752453557\n",
      "255 0.16070450087\n",
      "256 0.160657107886\n",
      "257 0.160610264034\n",
      "258 0.160563958758\n",
      "259 0.160518181547\n",
      "260 0.160472921982\n",
      "261 0.16042816977\n",
      "262 0.160383914777\n",
      "263 0.16034014706\n",
      "264 0.160296856902\n",
      "265 0.160254034837\n",
      "266 0.160211671677\n",
      "267 0.160169758538\n",
      "268 0.160128286858\n",
      "269 0.160087248413\n",
      "270 0.160046635331\n",
      "271 0.160006440094\n",
      "272 0.159966655534\n",
      "273 0.15992727483\n",
      "274 0.159888291482\n",
      "275 0.159849699296\n",
      "276 0.159811492342\n",
      "277 0.159773664926\n",
      "278 0.15973621155\n",
      "279 0.159699126866\n",
      "280 0.159662405644\n",
      "281 0.159626042735\n",
      "282 0.159590033039\n",
      "283 0.159554371484\n",
      "284 0.159519053008\n",
      "285 0.159484072546\n",
      "286 0.159449425028\n",
      "287 0.159415105377\n",
      "288 0.159381108512\n",
      "289 0.159347429355\n",
      "290 0.159314062842\n",
      "291 0.15928100393\n",
      "292 0.159248247607\n",
      "293 0.159215788906\n",
      "294 0.15918362291\n",
      "295 0.159151744765\n",
      "296 0.159120149685\n",
      "297 0.159088832963\n",
      "298 0.159057789974\n",
      "299 0.159027016182\n",
      "300 0.15899650714\n",
      "301 0.158966258501\n",
      "302 0.158936266012\n",
      "303 0.15890652552\n",
      "304 0.158877032972\n",
      "305 0.158847784413\n",
      "306 0.158818775987\n",
      "307 0.158790003936\n",
      "308 0.158761464597\n",
      "309 0.158733154403\n",
      "310 0.158705069875\n",
      "311 0.158677207625\n",
      "312 0.158649564353\n",
      "313 0.15862213684\n",
      "314 0.158594921949\n",
      "315 0.15856791662\n",
      "316 0.158541117869\n",
      "317 0.158514522783\n",
      "318 0.158488128519\n",
      "319 0.158461932299\n",
      "320 0.158435931409\n",
      "321 0.158410123198\n",
      "322 0.158384505069\n",
      "323 0.158359074486\n",
      "324 0.158333828964\n",
      "325 0.15830876607\n",
      "326 0.158283883421\n",
      "327 0.158259178681\n",
      "328 0.158234649563\n",
      "329 0.158210293821\n",
      "330 0.158186109255\n",
      "331 0.158162093704\n",
      "332 0.158138245049\n",
      "333 0.15811456121\n",
      "334 0.158091040146\n",
      "335 0.158067679853\n",
      "336 0.158044478362\n",
      "337 0.158021433741\n",
      "338 0.157998544092\n",
      "339 0.157975807553\n",
      "340 0.157953222293\n",
      "341 0.157930786516\n",
      "342 0.157908498455\n",
      "343 0.157886356378\n",
      "344 0.157864358582\n",
      "345 0.157842503395\n",
      "346 0.157820789174\n",
      "347 0.157799214308\n",
      "348 0.157777777209\n",
      "349 0.157756476323\n",
      "350 0.157735310119\n",
      "351 0.157714277093\n",
      "352 0.157693375768\n",
      "353 0.157672604691\n",
      "354 0.157651962433\n",
      "355 0.157631447589\n",
      "356 0.157611058775\n",
      "357 0.15759079463\n",
      "358 0.157570653812\n",
      "359 0.157550635001\n",
      "360 0.157530736893\n",
      "361 0.157510958204\n",
      "362 0.157491297667\n",
      "363 0.157471754029\n",
      "364 0.157452326055\n",
      "365 0.157433012522\n",
      "366 0.157413812223\n",
      "367 0.157394723963\n",
      "368 0.157375746557\n",
      "369 0.157356878836\n",
      "370 0.157338119639\n",
      "371 0.157319467814\n",
      "372 0.157300922223\n",
      "373 0.157282481735\n",
      "374 0.157264145227\n",
      "375 0.157245911588\n",
      "376 0.157227779712\n",
      "377 0.157209748504\n",
      "378 0.157191816876\n",
      "379 0.157173983748\n",
      "380 0.157156248048\n",
      "381 0.157138608713\n",
      "382 0.157121064685\n",
      "383 0.157103614917\n",
      "384 0.157086258369\n",
      "385 0.157068994009\n",
      "386 0.157051820814\n",
      "387 0.157034737768\n",
      "388 0.157017743864\n",
      "389 0.157000838106\n",
      "390 0.156984019504\n",
      "391 0.156967287079\n",
      "392 0.156950639862\n",
      "393 0.156934076891\n",
      "394 0.156917597219\n",
      "395 0.156901199904\n",
      "396 0.156884884019\n",
      "397 0.156868648645\n",
      "398 0.156852492877\n",
      "399 0.156836415821\n",
      "400 0.156820416593\n",
      "401 0.156804494326\n",
      "402 0.156788648162\n",
      "403 0.15677287726\n",
      "404 0.15675718079\n",
      "405 0.156741557938\n",
      "406 0.156726007905\n",
      "407 0.156710529909\n",
      "408 0.156695123182\n",
      "409 0.156679786974\n",
      "410 0.156664520552\n",
      "411 0.156649323201\n",
      "412 0.156634194225\n",
      "413 0.156619132946\n",
      "414 0.156604138709\n",
      "415 0.156589210874\n",
      "416 0.156574348827\n",
      "417 0.156559551973\n",
      "418 0.156544819738\n",
      "419 0.156530151571\n",
      "420 0.156515546946\n",
      "421 0.156501005355\n",
      "422 0.156486526316\n",
      "423 0.156472109369\n",
      "424 0.156457754078\n",
      "425 0.156443460028\n",
      "426 0.156429226828\n",
      "427 0.156415054107\n",
      "428 0.156400941518\n",
      "429 0.156386888732\n",
      "430 0.156372895442\n",
      "431 0.156358961358\n",
      "432 0.156345086211\n",
      "433 0.156331269745\n",
      "434 0.156317511723\n",
      "435 0.15630381192\n",
      "436 0.156290170123\n",
      "437 0.156276586134\n",
      "438 0.156263059762\n",
      "439 0.156249590824\n",
      "440 0.156236179147\n",
      "441 0.15622282456\n",
      "442 0.156209526898\n",
      "443 0.156196285998\n",
      "444 0.156183101699\n",
      "445 0.15616997384\n",
      "446 0.156156902258\n",
      "447 0.156143886788\n",
      "448 0.156130927263\n",
      "449 0.156118023511\n",
      "450 0.156105175357\n",
      "451 0.156092382618\n",
      "452 0.156079645108\n",
      "453 0.156066962632\n",
      "454 0.156054334992\n",
      "455 0.156041761981\n",
      "456 0.156029243385\n",
      "457 0.156016778986\n",
      "458 0.156004368556\n",
      "459 0.155992011864\n",
      "460 0.155979708671\n",
      "461 0.155967458731\n",
      "462 0.155955261797\n",
      "463 0.155943117612\n",
      "464 0.155931025918\n",
      "465 0.155918986451\n",
      "466 0.155906998944\n",
      "467 0.155895063127\n",
      "468 0.155883178727\n",
      "469 0.15587134547\n",
      "470 0.155859563078\n",
      "471 0.155847831274\n",
      "472 0.155836149779\n",
      "473 0.155824518314\n",
      "474 0.1558129366\n",
      "475 0.155801404359\n",
      "476 0.155789921311\n",
      "477 0.155778487181\n",
      "478 0.155767101692\n",
      "479 0.155755764571\n",
      "480 0.155744475546\n",
      "481 0.155733234347\n",
      "482 0.155722040706\n",
      "483 0.155710894358\n",
      "484 0.15569979504\n",
      "485 0.155688742492\n",
      "486 0.155677736458\n",
      "487 0.155666776683\n",
      "488 0.155655862917\n",
      "489 0.155644994912\n",
      "490 0.155634172422\n",
      "491 0.155623395207\n",
      "492 0.155612663027\n",
      "493 0.155601975648\n",
      "494 0.155591332839\n",
      "495 0.155580734369\n",
      "496 0.155570180013\n",
      "497 0.155559669548\n",
      "498 0.155549202755\n",
      "499 0.155538779416\n",
      "500 0.155528399318\n",
      "501 0.155518062249\n",
      "502 0.155507768\n",
      "503 0.155497516365\n",
      "504 0.15548730714\n",
      "505 0.155477140122\n",
      "506 0.155467015112\n",
      "507 0.155456931911\n",
      "508 0.155446890322\n",
      "509 0.15543689015\n",
      "510 0.155426931198\n",
      "511 0.155417013273\n",
      "512 0.155407136181\n",
      "513 0.155397299726\n",
      "514 0.155387503715\n",
      "515 0.155377747949\n",
      "516 0.155368032232\n",
      "517 0.155358356362\n",
      "518 0.155348720136\n",
      "519 0.155339123346\n",
      "520 0.155329565779\n",
      "521 0.155320047219\n",
      "522 0.155310567439\n",
      "523 0.155301126207\n",
      "524 0.155291723281\n",
      "525 0.155282358406\n",
      "526 0.155273031318\n",
      "527 0.155263741735\n",
      "528 0.155254489362\n",
      "529 0.155245273881\n",
      "530 0.155236094956\n",
      "531 0.155226952224\n",
      "532 0.155217845298\n",
      "533 0.155208773756\n",
      "534 0.155199737142\n",
      "535 0.155190734964\n",
      "536 0.155181766682\n",
      "537 0.15517283171\n",
      "538 0.155163929405\n",
      "539 0.155155059067\n",
      "540 0.155146219925\n",
      "541 0.155137411138\n",
      "542 0.155128631782\n",
      "543 0.155119880845\n",
      "544 0.155111157221\n",
      "545 0.155102459698\n",
      "546 0.155093786954\n",
      "547 0.155085137551\n",
      "548 0.155076509924\n",
      "549 0.155067902384\n",
      "550 0.155059313109\n",
      "551 0.155050740148\n",
      "552 0.155042181428\n",
      "553 0.155033634756\n",
      "554 0.155025097845\n",
      "555 0.155016568331\n",
      "556 0.155008043812\n",
      "557 0.154999521888\n",
      "558 0.154991000228\n",
      "559 0.154982476638\n",
      "560 0.154973949152\n",
      "561 0.15496541614\n",
      "562 0.154956876427\n",
      "563 0.154948329425\n",
      "564 0.154939775274\n",
      "565 0.154931214976\n",
      "566 0.154922650517\n",
      "567 0.154914084965\n",
      "568 0.154905522512\n",
      "569 0.154896968461\n",
      "570 0.154888429137\n",
      "571 0.154879911702\n",
      "572 0.1548714239\n",
      "573 0.154862973728\n",
      "574 0.154854569078\n",
      "575 0.154846217372\n",
      "576 0.154837925249\n",
      "577 0.154829698329\n",
      "578 0.154821541066\n",
      "579 0.154813456701\n",
      "580 0.154805447306\n",
      "581 0.154797513879\n",
      "582 0.154789656494\n",
      "583 0.154781874458\n",
      "584 0.154774166473\n",
      "585 0.15476653079\n",
      "586 0.15475896534\n",
      "587 0.154751467851\n",
      "588 0.15474403594\n",
      "589 0.154736667191\n",
      "590 0.154729359207\n",
      "591 0.154722109656\n",
      "592 0.154714916301\n",
      "593 0.154707777021\n",
      "594 0.154700689823\n",
      "595 0.154693652844\n",
      "596 0.15468666436\n",
      "597 0.154679722779\n",
      "598 0.154672826639\n",
      "599 0.1546659746\n",
      "600 0.154659165439\n",
      "601 0.154652398041\n",
      "602 0.154645671391\n",
      "603 0.154638984569\n",
      "604 0.154632336736\n",
      "605 0.154625727134\n",
      "606 0.154619155075\n",
      "607 0.154612619934\n",
      "608 0.154606121144\n",
      "609 0.154599658192\n",
      "610 0.154593230609\n",
      "611 0.154586837971\n",
      "612 0.154580479891\n",
      "613 0.154574156016\n",
      "614 0.154567866024\n",
      "615 0.154561609621\n",
      "616 0.154555386536\n",
      "617 0.154549196522\n",
      "618 0.15454303935\n",
      "619 0.154536914808\n",
      "620 0.154530822703\n",
      "621 0.154524762852\n",
      "622 0.154518735088\n",
      "623 0.154512739253\n",
      "624 0.1545067752\n",
      "625 0.154500842792\n",
      "626 0.154494941898\n",
      "627 0.154489072396\n",
      "628 0.154483234169\n",
      "629 0.154477427109\n",
      "630 0.154471651109\n",
      "631 0.154465906071\n",
      "632 0.154460191898\n",
      "633 0.154454508499\n",
      "634 0.154448855785\n",
      "635 0.154443233672\n",
      "636 0.154437642079\n",
      "637 0.154432080925\n",
      "638 0.154426550135\n",
      "639 0.154421049633\n",
      "640 0.154415579348\n",
      "641 0.15441013921\n",
      "642 0.154404729148\n",
      "643 0.154399349097\n",
      "644 0.154393998992\n",
      "645 0.154388678767\n",
      "646 0.15438338836\n",
      "647 0.154378127709\n",
      "648 0.154372896753\n",
      "649 0.154367695434\n",
      "650 0.154362523691\n",
      "651 0.154357381467\n",
      "652 0.154352268704\n",
      "653 0.154347185345\n",
      "654 0.154342131334\n",
      "655 0.154337106615\n",
      "656 0.154332111132\n",
      "657 0.15432714483\n",
      "658 0.154322207654\n",
      "659 0.154317299548\n",
      "660 0.154312420458\n",
      "661 0.154307570327\n",
      "662 0.1543027491\n",
      "663 0.154297956722\n",
      "664 0.154293193137\n",
      "665 0.154288458288\n",
      "666 0.154283752118\n",
      "667 0.154279074569\n",
      "668 0.154274425583\n",
      "669 0.154269805102\n",
      "670 0.154265213065\n",
      "671 0.154260649411\n",
      "672 0.15425611408\n",
      "673 0.154251607008\n",
      "674 0.154247128133\n",
      "675 0.154242677388\n",
      "676 0.154238254709\n",
      "677 0.154233860029\n",
      "678 0.154229493278\n",
      "679 0.154225154388\n",
      "680 0.154220843288\n",
      "681 0.154216559905\n",
      "682 0.154212304166\n",
      "683 0.154208075997\n",
      "684 0.15420387532\n",
      "685 0.154199702058\n",
      "686 0.154195556133\n",
      "687 0.154191437463\n",
      "688 0.154187345968\n",
      "689 0.154183281564\n",
      "690 0.154179244166\n",
      "691 0.154175233689\n",
      "692 0.154171250046\n",
      "693 0.154167293147\n",
      "694 0.154163362905\n",
      "695 0.154159459226\n",
      "696 0.154155582021\n",
      "697 0.154151731194\n",
      "698 0.154147906652\n",
      "699 0.154144108299\n",
      "700 0.154140336038\n",
      "701 0.154136589772\n",
      "702 0.154132869402\n",
      "703 0.154129174829\n",
      "704 0.154125505952\n",
      "705 0.15412186267\n",
      "706 0.154118244881\n",
      "707 0.154114652482\n",
      "708 0.154111085369\n",
      "709 0.154107543439\n",
      "710 0.154104026587\n",
      "711 0.154100534709\n",
      "712 0.154097067697\n",
      "713 0.154093625448\n",
      "714 0.154090207855\n",
      "715 0.154086814811\n",
      "716 0.154083446212\n",
      "717 0.15408010195\n",
      "718 0.154076781921\n",
      "719 0.154073486018\n",
      "720 0.154070214137\n",
      "721 0.154066966174\n",
      "722 0.154063742026\n",
      "723 0.15406054159\n",
      "724 0.154057364766\n",
      "725 0.154054211453\n",
      "726 0.154051081555\n",
      "727 0.154047974977\n",
      "728 0.154044891624\n",
      "729 0.154041831407\n",
      "730 0.15403879424\n",
      "731 0.154035780037\n",
      "732 0.15403278872\n",
      "733 0.154029820214\n",
      "734 0.154026874449\n",
      "735 0.154023951359\n",
      "736 0.154021050886\n",
      "737 0.154018172978\n",
      "738 0.154015317591\n",
      "739 0.154012484686\n",
      "740 0.154009674235\n",
      "741 0.154006886218\n",
      "742 0.154004120624\n",
      "743 0.154001377453\n",
      "744 0.153998656714\n",
      "745 0.153995958429\n",
      "746 0.153993282629\n",
      "747 0.153990629357\n",
      "748 0.153987998668\n",
      "749 0.153985390629\n",
      "750 0.153982805315\n",
      "751 0.153980242816\n",
      "752 0.153977703228\n",
      "753 0.153975186659\n",
      "754 0.153972693223\n",
      "755 0.153970223042\n",
      "756 0.153967776243\n",
      "757 0.153965352953\n",
      "758 0.153962953304\n",
      "759 0.153960577425\n",
      "760 0.153958225441\n",
      "761 0.153955897471\n",
      "762 0.153953593626\n",
      "763 0.153951314007\n",
      "764 0.153949058699\n",
      "765 0.153946827774\n",
      "766 0.153944621286\n",
      "767 0.15394243927\n",
      "768 0.153940281742\n",
      "769 0.153938148696\n",
      "770 0.153936040105\n",
      "771 0.153933955921\n",
      "772 0.153931896076\n",
      "773 0.15392986048\n",
      "774 0.153927849026\n",
      "775 0.153925861589\n",
      "776 0.153923898026\n",
      "777 0.153921958182\n",
      "778 0.153920041886\n",
      "779 0.153918148961\n",
      "780 0.153916279216\n",
      "781 0.153914432457\n",
      "782 0.153912608481\n",
      "783 0.153910807085\n",
      "784 0.153909028061\n",
      "785 0.1539072712\n",
      "786 0.153905536297\n",
      "787 0.153903823146\n",
      "788 0.153902131544\n",
      "789 0.153900461293\n",
      "790 0.153898812198\n",
      "791 0.153897184072\n",
      "792 0.153895576732\n",
      "793 0.153893990002\n",
      "794 0.153892423713\n",
      "795 0.153890877706\n",
      "796 0.153889351825\n",
      "797 0.153887845926\n",
      "798 0.153886359872\n",
      "799 0.153884893535\n",
      "800 0.153883446794\n",
      "801 0.153882019538\n",
      "802 0.153880611663\n",
      "803 0.153879223076\n",
      "804 0.153877853689\n",
      "805 0.153876503426\n",
      "806 0.153875172215\n",
      "807 0.153873859996\n",
      "808 0.153872566715\n",
      "809 0.153871292325\n",
      "810 0.153870036787\n",
      "811 0.153868800069\n",
      "812 0.153867582148\n",
      "813 0.153866383006\n",
      "814 0.15386520263\n",
      "815 0.153864041017\n",
      "816 0.153862898168\n",
      "817 0.153861774091\n",
      "818 0.1538606688\n",
      "819 0.153859582313\n",
      "820 0.153858514656\n",
      "821 0.153857465859\n",
      "822 0.153856435957\n",
      "823 0.153855424992\n",
      "824 0.153854433008\n",
      "825 0.153853460058\n",
      "826 0.153852506197\n",
      "827 0.153851571484\n",
      "828 0.153850655985\n",
      "829 0.153849759769\n",
      "830 0.153848882909\n",
      "831 0.153848025482\n",
      "832 0.15384718757\n",
      "833 0.153846369257\n",
      "834 0.153845570631\n",
      "835 0.153844791783\n",
      "836 0.153844032807\n",
      "837 0.153843293799\n",
      "838 0.153842574856\n",
      "839 0.153841876077\n",
      "840 0.153841197563\n",
      "841 0.153840539415\n",
      "842 0.153839901731\n",
      "843 0.153839284612\n",
      "844 0.153838688155\n",
      "845 0.153838112457\n",
      "846 0.153837557611\n",
      "847 0.153837023707\n",
      "848 0.15383651083\n",
      "849 0.153836019061\n",
      "850 0.153835548478\n",
      "851 0.153835099149\n",
      "852 0.153834671138\n",
      "853 0.153834264503\n",
      "854 0.153833879293\n",
      "855 0.153833515549\n",
      "856 0.153833173307\n",
      "857 0.15383285259\n",
      "858 0.153832553417\n",
      "859 0.153832275797\n",
      "860 0.153832019729\n",
      "861 0.153831785206\n",
      "862 0.153831572212\n",
      "863 0.153831380721\n",
      "864 0.153831210701\n",
      "865 0.153831062112\n",
      "866 0.153830934907\n",
      "867 0.15383082903\n",
      "868 0.15383074442\n",
      "869 0.15383068101\n",
      "870 0.153830638725\n",
      "871 0.153830617487\n",
      "872 0.153830617211\n",
      "873 0.153830637807\n",
      "874 0.153830679184\n",
      "875 0.153830741242\n",
      "876 0.153830823882\n",
      "877 0.153830926999\n",
      "878 0.153831050485\n",
      "879 0.153831194232\n",
      "880 0.153831358128\n",
      "881 0.15383154206\n",
      "882 0.153831745912\n",
      "883 0.153831969569\n",
      "884 0.153832212914\n",
      "885 0.15383247583\n",
      "886 0.1538327582\n",
      "887 0.153833059905\n",
      "888 0.15383338083\n",
      "889 0.153833720859\n",
      "890 0.153834079876\n",
      "891 0.153834457768\n",
      "892 0.153834854423\n",
      "893 0.153835269731\n",
      "894 0.153835703586\n",
      "895 0.153836155882\n",
      "896 0.153836626518\n",
      "897 0.153837115395\n",
      "898 0.15383762242\n",
      "899 0.153838147502\n",
      "900 0.153838690555\n",
      "901 0.153839251499\n",
      "902 0.153839830258\n",
      "903 0.153840426761\n",
      "904 0.153841040946\n",
      "905 0.153841672753\n",
      "906 0.153842322133\n",
      "907 0.153842989041\n",
      "908 0.153843673441\n",
      "909 0.153844375303\n",
      "910 0.153845094606\n",
      "911 0.153845831338\n",
      "912 0.153846585493\n",
      "913 0.153847357074\n",
      "914 0.153848146094\n",
      "915 0.153848952573\n",
      "916 0.153849776539\n",
      "917 0.153850618029\n",
      "918 0.153851477089\n",
      "919 0.153852353771\n",
      "920 0.153853248134\n",
      "921 0.153854160248\n",
      "922 0.153855090186\n",
      "923 0.153856038027\n",
      "924 0.153857003858\n",
      "925 0.15385798777\n",
      "926 0.153858989857\n",
      "927 0.153860010219\n",
      "928 0.153861048957\n",
      "929 0.153862106175\n",
      "930 0.153863181979\n",
      "931 0.153864276474\n",
      "932 0.153865389768\n",
      "933 0.153866521966\n",
      "934 0.153867673174\n",
      "935 0.153868843493\n",
      "936 0.153870033026\n",
      "937 0.153871241871\n",
      "938 0.153872470121\n",
      "939 0.153873717869\n",
      "940 0.153874985203\n",
      "941 0.153876272204\n",
      "942 0.153877578953\n",
      "943 0.153878905523\n",
      "944 0.153880251983\n",
      "945 0.153881618399\n",
      "946 0.15388300483\n",
      "947 0.15388441133\n",
      "948 0.153885837949\n",
      "949 0.153887284732\n",
      "950 0.15388875172\n",
      "951 0.153890238947\n",
      "952 0.153891746443\n",
      "953 0.153893274237\n",
      "954 0.153894822348\n",
      "955 0.153896390794\n",
      "956 0.15389797959\n",
      "957 0.153899588744\n",
      "958 0.153901218263\n",
      "959 0.153902868148\n",
      "960 0.153904538398\n",
      "961 0.153906229009\n",
      "962 0.153907939973\n",
      "963 0.153909671279\n",
      "964 0.153911422915\n",
      "965 0.153913194863\n",
      "966 0.153914987106\n",
      "967 0.153916799623\n",
      "968 0.153918632392\n",
      "969 0.153920485386\n",
      "970 0.15392235858\n",
      "971 0.153924251946\n",
      "972 0.153926165454\n",
      "973 0.153928099072\n",
      "974 0.153930052768\n",
      "975 0.153932026509\n",
      "976 0.15393402026\n",
      "977 0.153936033987\n",
      "978 0.153938067653\n",
      "979 0.153940121223\n",
      "980 0.153942194659\n",
      "981 0.153944287925\n",
      "982 0.153946400983\n",
      "983 0.153948533796\n",
      "984 0.153950686327\n",
      "985 0.15395285854\n",
      "986 0.153955050396\n",
      "987 0.15395726186\n",
      "988 0.153959492896\n",
      "989 0.153961743467\n",
      "990 0.153964013539\n",
      "991 0.153966303077\n",
      "992 0.153968612045\n",
      "993 0.153970940411\n",
      "994 0.153973288142\n",
      "995 0.153975655203\n",
      "996 0.153978041564\n",
      "997 0.15398044719\n",
      "998 0.153982872052\n",
      "999 0.153985316117\n",
      "0.7249401300034212\n"
     ]
    }
   ],
   "source": [
    "# Initialising Neural Network\n",
    "neural_network = NeuralNetwork1()\n",
    "\n",
    "#print (\"Starting weights Layer 1: \")\n",
    "#print (neural_network.weights1)\n",
    "#print (\"\\nStarting weights Layer 2: \")\n",
    "#print (neural_network.weights2)\n",
    "#print (\"\\nStarting weights Layer 3: \")\n",
    "#print (neural_network.weights3)\n",
    "\n",
    "# Training Set\n",
    "training_set_inputs = np.array(trn_features)\n",
    "training_set_outputs = np.array(trn_y.T)\n",
    "\n",
    "#fweights1,fweights2,fweights3=neural_network.train(training_set_inputs, training_set_outputs.T,100,0.00003)\n",
    "fweights1,fweights2=neural_network.train(training_set_inputs, training_set_outputs.T,1000,0.00001)\n",
    "\n",
    "#print (\"\\nNew weights Layer 1 after training:\")\n",
    "#print (neural_network.weights1)\n",
    "#print (\"\\nNew weights Layer 2 after training: \")\n",
    "#print (neural_network.weights2)\n",
    "#print (\"\\nNew weights Layer 3 after training: \")\n",
    "#print (neural_network.weights3)\n",
    "\n",
    "# Validation\n",
    "after_l1 = tanh(np.dot(vld_features,fweights1))\n",
    "after_l2 = tanh(np.dot(after_l1,fweights2))\n",
    "#print(\"after_l3\")\n",
    "#print(after_l3)\n",
    "predicted = []\n",
    "\n",
    "after_l2 = after_l2.ravel()\n",
    "#print(after_l3)\n",
    "\n",
    "#Threshholding values\n",
    "for i in range(len(after_l2)):\n",
    "    if after_l2[i] > 0:\n",
    "        predicted.append([1])\n",
    "    else:\n",
    "        predicted.append([0])\n",
    "\n",
    "#Validating accuracy of Neural network results\n",
    "vld_y = np.array(vld_y).ravel()\n",
    "#print(vld_y)\n",
    "#np.savetxt(\"data/vld_y.csv\",vld_y,delimiter=',',fmt=\"%d\",header=\"id,salary\",comments ='')\n",
    "c=0\n",
    "for i in range(len(predicted)):\n",
    "    if(predicted[i] == vld_y[i]):\n",
    "        c+=1\n",
    "vld_acur=c/len(predicted)\n",
    "print(vld_acur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting final results from test data\n",
    "after_l1 = tanh(np.dot(tst_features,fweights1))\n",
    "after_l2 = tanh(np.dot(after_l1,fweights2))\n",
    "after_l3 = tanh(np.dot(after_l2,fweights3))\n",
    "print(\"after_l3\")\n",
    "print(after_l3)\n",
    "predicted = []\n",
    "after_l3 = after_l3.ravel()\n",
    "\n",
    "result=np.column_stack((test_ids,after_l3))\n",
    "np.savetxt(\"data/output.csv\",result,delimiter=',',fmt=\"%d,%f\",header=\"id,salary\",comments ='')\n",
    "print(after_l3)\n",
    "for i in range(len(after_l3)):\n",
    "    if after_l3[i] > 0:\n",
    "        predicted.append(1)\n",
    "    else:\n",
    "        predicted.append(0)\n",
    "print(\"predicted\")\n",
    "print(predicted)\n",
    "result=np.column_stack((test_ids,(after_l3)))\n",
    "\n",
    "np.savetxt(\"data/output.csv\",result,delimiter=',',fmt=\"%d,%f\",header=\"id,salary\",comments ='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_l2\n",
      "[[-0.13650391]\n",
      " [-0.318587  ]\n",
      " [ 0.33493801]\n",
      " ..., \n",
      " [-0.23909666]\n",
      " [-0.25220552]\n",
      " [ 0.25789659]]\n",
      "[-0.13650391 -0.318587    0.33493801 ..., -0.23909666 -0.25220552\n",
      "  0.25789659]\n",
      "predicted\n",
      "[0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Getting final results from test data\n",
    "after_l1 = tanh(np.dot(tst_features,fweights1))\n",
    "after_l2 = tanh(np.dot(after_l1,fweights2))\n",
    "print(\"after_l2\")\n",
    "print(after_l2)\n",
    "predicted = []\n",
    "after_l2 = after_l2.ravel()\n",
    "\n",
    "result=np.column_stack((test_ids,after_l2))\n",
    "np.savetxt(\"data/output.csv\",result,delimiter=',',fmt=\"%d,%f\",header=\"id,salary\",comments ='')\n",
    "print(after_l2)\n",
    "for i in range(len(after_l2)):\n",
    "    if after_l2[i] > 0:\n",
    "        predicted.append(1)\n",
    "    else:\n",
    "        predicted.append(0)\n",
    "print(\"predicted\")\n",
    "print(predicted)\n",
    "result=np.column_stack((test_ids,(after_l2)))\n",
    "\n",
    "np.savetxt(\"data/output.csv\",result,delimiter=',',fmt=\"%d,%f\",header=\"id,salary\",comments ='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
