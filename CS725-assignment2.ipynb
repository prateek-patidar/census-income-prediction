{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import exp,array,random,dot\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(matrix):\n",
    "    n = matrix.shape[1]\n",
    "    norm_matrix = matrix\n",
    "\n",
    "    for i in range(n):\n",
    "        col_mean = np.mean(matrix[:,i])\n",
    "        col_stdv = np.std(matrix[:,i])\n",
    "    norm_matrix[:,i] = (norm_matrix[:,i] - col_mean) / col_stdv\n",
    "    return norm_matrix\n",
    "\n",
    "def normalize1(matrix):\n",
    "    return (matrix - matrix.mean()) / matrix.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading data from csv\n",
    "data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classifier data entries to dummy coloumns\n",
    "#data = pd.get_dummies(data)\n",
    "#data.to_csv(\"data/data_afterdum.csv\", sep=',',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27281 11692\n"
     ]
    }
   ],
   "source": [
    "# Spliting the data into Training data and Validation data\n",
    "train, validate = np.split(data.sample(frac=1), [int(.7*len(data))])\n",
    "print(len(train),len(validate))\n",
    "#print(train)\n",
    "#print(validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27281, 45)\n"
     ]
    }
   ],
   "source": [
    "trn_ids = train[['id']]\n",
    "#print(train_ids)\n",
    "trn_y = train[['salary']]\n",
    "#print(train_y)\n",
    "trn_features = train.drop('id', axis=1)\n",
    "trn_features = trn_features.drop('salary', axis=1)\n",
    "trn_features = trn_features.drop('race', axis=1)\n",
    "trn_features = trn_features.drop('native-country', axis=1)\n",
    "trn_features = trn_features.drop('education', axis=1)#correlated\n",
    "trn_features = pd.get_dummies(trn_features)\n",
    "trn_features = normalize1(trn_features)\n",
    "print(trn_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11692, 45)\n"
     ]
    }
   ],
   "source": [
    "vld_ids = validate[['id']]\n",
    "#print(valid_ids)\n",
    "vld_y = validate[['salary']]\n",
    "#print(train_y)\n",
    "vld_features = validate.drop('id', axis=1)\n",
    "vld_features = vld_features.drop('salary', axis=1)\n",
    "vld_features = vld_features.drop('native-country', axis=1)\n",
    "vld_features = vld_features.drop('race', axis=1)\n",
    "vld_features = vld_features.drop('education', axis=1)\n",
    "vld_features = pd.get_dummies(vld_features)\n",
    "vld_features = normalize1(vld_features)\n",
    "print(vld_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6878, 45)\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "test_ids = test_data['id']\n",
    "#print(test_ids)\n",
    "tst_features = test_data.drop('id', axis=1)\n",
    "tst_features = tst_features.drop('native-country', axis=1)\n",
    "tst_features = tst_features.drop('race', axis=1)\n",
    "tst_features = tst_features.drop('education', axis=1)\n",
    "tst_features = pd.get_dummies(tst_features)\n",
    "tst_features = normalize1(tst_features)\n",
    "print(tst_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now Neural Network implementation starts\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x))\n",
    "\n",
    "def derv_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "def derv_tanh(x):\n",
    "        return (1.0 - np.square(np.tanh(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Number of nodes in each Layer\n",
    "        l1_nodes=45\n",
    "        l2_nodes=30\n",
    "        l3_nodes=30\n",
    "        l4_nodes=1\n",
    "\n",
    "        # Assigning random weights to Neural Network\n",
    "        self.weights1 = 2 * random.random((l1_nodes, l2_nodes)) -1\n",
    "        self.weights2 = 2 * random.random((l2_nodes, l3_nodes)) -1\n",
    "        self.weights3 = 2 * random.random((l3_nodes, l4_nodes)) -1\n",
    "\n",
    "    # Activation functions\n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def derv_tanh(self,x):\n",
    "        return (1.0 - (np.tanh(x))**2)\n",
    "\n",
    "    # Train neural network using backpropogation\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations,eta):\n",
    "        for i in range(number_of_training_iterations):\n",
    "            # print(i)\n",
    "            \n",
    "            # Forward Pass\n",
    "            a2 = self.tanh(dot(training_set_inputs, self.weights1))\n",
    "            a3 = self.tanh(dot(a2, self.weights2))\n",
    "            output = self.tanh(dot(a3, self.weights3))\n",
    "\n",
    "            # Backpopogation Pass\n",
    "            # Error for each layer\n",
    "            del4 = (training_set_outputs - output)*self.derv_tanh(output)\n",
    "            print(i,np.mean(np.square(del4)))\n",
    "            del3 = dot(self.weights3, del4.T)*(self.derv_tanh(a3).T)\n",
    "            del2 = dot(self.weights2, del3)*(self.derv_tanh(a2).T)\n",
    "\n",
    "            # Adjustments (gradients) for each layer\n",
    "            adjustment3 = eta*dot(a3.T, del4)\n",
    "            adjustment2 = eta*dot(a2.T, del3.T)\n",
    "            adjustment1 = eta*dot(training_set_inputs.T, del2.T)\n",
    "\n",
    "            # Adjusting weights accordingly\n",
    "            self.weights1 += adjustment1\n",
    "            self.weights2 += adjustment2\n",
    "            self.weights3 += adjustment3\n",
    "\n",
    "        return self.weights1,self.weights2,self.weights3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.248649201602\n",
      "1 0.189103787201\n",
      "2 0.210361197383\n",
      "3 0.186884772336\n",
      "4 0.204432964145\n",
      "5 0.184958055687\n",
      "6 0.197448300748\n",
      "7 0.181145229779\n",
      "8 0.189911967933\n",
      "9 0.176294038928\n",
      "10 0.182998397402\n",
      "11 0.171484638459\n",
      "12 0.177066321584\n",
      "13 0.166837969761\n",
      "14 0.171904799155\n",
      "15 0.162365446981\n",
      "16 0.167447792696\n",
      "17 0.158194562286\n",
      "18 0.163765899703\n",
      "19 0.154527898058\n",
      "20 0.161025091571\n",
      "21 0.151670598221\n",
      "22 0.159457032367\n",
      "23 0.150055690556\n",
      "24 0.159202132197\n",
      "25 0.150031213935\n",
      "26 0.15987657092\n",
      "27 0.151225261618\n",
      "28 0.160267864947\n",
      "29 0.152280831058\n",
      "30 0.159154945873\n",
      "31 0.152059825707\n",
      "32 0.156458684435\n",
      "33 0.15065237888\n",
      "34 0.152938266223\n",
      "35 0.148746659876\n",
      "36 0.149316821501\n",
      "37 0.146888994593\n",
      "38 0.145984443233\n",
      "39 0.145368301226\n",
      "40 0.143086531806\n",
      "41 0.144325507401\n",
      "42 0.140657287887\n",
      "43 0.143844046577\n",
      "44 0.138704985507\n",
      "45 0.143980797918\n",
      "46 0.137246402547\n",
      "47 0.144725489065\n",
      "48 0.136280411258\n",
      "49 0.145873868225\n",
      "50 0.135686528535\n",
      "51 0.146890638844\n",
      "52 0.135129819546\n",
      "53 0.147032404732\n",
      "54 0.134191164943\n",
      "55 0.145856313824\n",
      "56 0.132714436921\n",
      "57 0.143573671684\n",
      "58 0.130924829328\n",
      "59 0.140788772853\n",
      "60 0.129174316477\n",
      "61 0.138047504583\n",
      "62 0.127694988655\n",
      "63 0.135653333217\n",
      "64 0.126560248988\n",
      "65 0.133709874214\n",
      "66 0.125752099111\n",
      "67 0.132216249788\n",
      "68 0.125226536539\n",
      "69 0.131135747673\n",
      "70 0.124947865503\n",
      "71 0.130428480909\n",
      "72 0.124897315069\n",
      "73 0.130057316377\n",
      "74 0.125063906486\n",
      "75 0.129975371183\n",
      "76 0.12542227785\n",
      "77 0.130103206223\n",
      "78 0.12590565806\n",
      "79 0.130311344684\n",
      "80 0.126393267261\n",
      "81 0.130431554666\n",
      "82 0.126734696536\n",
      "83 0.130307352044\n",
      "84 0.126808210579\n",
      "85 0.129855986007\n",
      "86 0.126571574218\n",
      "87 0.129093735022\n",
      "88 0.12606591324\n",
      "89 0.128108865413\n",
      "90 0.125378685628\n",
      "91 0.127011693448\n",
      "92 0.124601143251\n",
      "93 0.125896557834\n",
      "94 0.123803826151\n",
      "95 0.124826962494\n",
      "96 0.123030981183\n",
      "97 0.123837095069\n",
      "98 0.122305103363\n",
      "99 0.122939783051\n",
      "0.5514026684912761\n"
     ]
    }
   ],
   "source": [
    "# Initialising Neural Network\n",
    "neural_network = NeuralNetwork()\n",
    "\n",
    "#print (\"Starting weights Layer 1: \")\n",
    "#print (neural_network.weights1)\n",
    "#print (\"\\nStarting weights Layer 2: \")\n",
    "#print (neural_network.weights2)\n",
    "#print (\"\\nStarting weights Layer 3: \")\n",
    "#print (neural_network.weights3)\n",
    "\n",
    "# Training Set\n",
    "training_set_inputs = np.array(trn_features)\n",
    "training_set_outputs = np.array(trn_y.T)\n",
    "\n",
    "fweights1,fweights2,fweights3=neural_network.train(training_set_inputs, training_set_outputs.T,100,0.00003)\n",
    "\n",
    "#print (\"\\nNew weights Layer 1 after training:\")\n",
    "#print (neural_network.weights1)\n",
    "#print (\"\\nNew weights Layer 2 after training: \")\n",
    "#print (neural_network.weights2)\n",
    "#print (\"\\nNew weights Layer 3 after training: \")\n",
    "#print (neural_network.weights3)\n",
    "\n",
    "# Validation\n",
    "after_l1 = tanh(np.dot(vld_features,fweights1))\n",
    "after_l2 = tanh(np.dot(after_l1,fweights2))\n",
    "after_l3 = tanh(np.dot(after_l2,fweights3))\n",
    "#print(\"after_l3\")\n",
    "#print(after_l3)\n",
    "predicted = []\n",
    "\n",
    "after_l3 = after_l3.ravel()\n",
    "#print(after_l3)\n",
    "\n",
    "#Threshholding values\n",
    "for i in range(len(after_l3)):\n",
    "    if after_l3[i] > 0:\n",
    "        predicted.append([1])\n",
    "    else:\n",
    "        predicted.append([0])\n",
    "\n",
    "#Validating accuracy of Neural network results\n",
    "vld_y = np.array(vld_y).ravel()\n",
    "#print(vld_y)\n",
    "#np.savetxt(\"data/vld_y.csv\",vld_y,delimiter=',',fmt=\"%d\",header=\"id,salary\",comments ='')\n",
    "c=0\n",
    "for i in range(len(predicted)):\n",
    "    if(predicted[i] == vld_y[i]):\n",
    "        c+=1\n",
    "vld_acur=c/len(predicted)\n",
    "print(vld_acur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting final results from test data\n",
    "after_l1 = tanh(np.dot(tst_features,fweights1))\n",
    "after_l2 = tanh(np.dot(after_l1,fweights2))\n",
    "after_l3 = tanh(np.dot(after_l2,fweights3))\n",
    "print(\"after_l3\")\n",
    "print(after_l3)\n",
    "predicted = []\n",
    "after_l3 = after_l3.ravel()\n",
    "\n",
    "result=np.column_stack((test_ids,after_l3))\n",
    "np.savetxt(\"data/output.csv\",result,delimiter=',',fmt=\"%d,%f\",header=\"id,salary\",comments ='')\n",
    "print(after_l3)\n",
    "for i in range(len(after_l3)):\n",
    "    if after_l3[i] > 0:\n",
    "        predicted.append(1)\n",
    "    else:\n",
    "        predicted.append(0)\n",
    "print(\"predicted\")\n",
    "print(predicted)\n",
    "result=np.column_stack((test_ids,(after_l3))\n",
    "\n",
    "np.savetxt(\"data/output.csv\",result,delimiter=',',fmt=\"%d,%f\",header=\"id,salary\",comments ='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
